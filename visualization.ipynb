{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5efcac-54fc-4418-a495-4ccc92e94214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ------------------------\n",
    "# 1. Read Data and Construct Graph\n",
    "# ------------------------\n",
    "filename = 'updated_chatgpt_reddit_comments.csv'\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "# Ensure the IDs are treated as strings\n",
    "data['comment_id'] = data['comment_id'].astype(str)\n",
    "data['comment_parent_id'] = data['comment_parent_id'].astype(str)\n",
    "\n",
    "# Extract columns\n",
    "comment_ids = data['comment_id']\n",
    "parent_ids = data['comment_parent_id']\n",
    "\n",
    "# Collect all unique nodes\n",
    "all_nodes = pd.unique(pd.concat([comment_ids, parent_ids]))\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(all_nodes)\n",
    "edges = list(zip(comment_ids, parent_ids))\n",
    "G.add_edges_from(edges)\n",
    "# Remove self loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "# ------------------------\n",
    "# 2. Plot the Graph\n",
    "# ------------------------\n",
    "# Use a spring layout (force-directed)\n",
    "pos = nx.spring_layout(G, seed=42)  # seed for reproducibility\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=50, ax=ax)\n",
    "nx.draw_networkx_edges(G, pos, ax=ax, arrows=False, alpha=0.3)\n",
    "ax.set_title('Comment Thread Graph')\n",
    "ax.axis('off')\n",
    "\n",
    "# Global storage for use in callback\n",
    "global_data = {\n",
    "    'G': G,\n",
    "    'data': data,\n",
    "    'pos': pos\n",
    "}\n",
    "\n",
    "# ------------------------\n",
    "# 3. Define Mouse Click Callback Function\n",
    "# ------------------------\n",
    "def on_click(event):\n",
    "    # Ignore clicks outside the axes\n",
    "    if event.xdata is None or event.ydata is None:\n",
    "        return\n",
    "    click_point = np.array([event.xdata, event.ydata])\n",
    "    \n",
    "    # Find the nearest node by comparing distances from click_point to node positions\n",
    "    pos_array = np.array(list(global_data['pos'].values()))\n",
    "    nodes = list(global_data['pos'].keys())\n",
    "    distances = np.linalg.norm(pos_array - click_point, axis=1)\n",
    "    min_dist = np.min(distances)\n",
    "    idx = np.argmin(distances)\n",
    "    \n",
    "    # Define a threshold (may need tuning depending on your layout scale)\n",
    "    threshold = 0.05\n",
    "    if min_dist > threshold:\n",
    "        print(\"Click was not close enough to any node.\")\n",
    "        return\n",
    "    \n",
    "    clicked_node = nodes[idx]\n",
    "    print(\"Clicked comment_id:\", clicked_node)\n",
    "    \n",
    "    # Retrieve corresponding serial_number from the data table (if available)\n",
    "    row = global_data['data'][global_data['data']['comment_id'] == clicked_node]\n",
    "    if not row.empty:\n",
    "        serial_number = row.iloc[0]['serial_number']\n",
    "        print(\"Serial number for clicked comment:\", serial_number)\n",
    "    else:\n",
    "        print(\"Clicked comment_id not found in data table.\")\n",
    "    \n",
    "    # ------------------------\n",
    "    # Find Descendant Nodes\n",
    "    # ------------------------\n",
    "    # Reverse the graph to follow child-to-parent edges\n",
    "    G_rev = global_data['G'].reverse(copy=False)\n",
    "    # Use networkx.descendants to get all nodes reachable from clicked_node\n",
    "    descendant_nodes = nx.descendants(G_rev, clicked_node)\n",
    "    print(\"Descendant nodes:\", descendant_nodes)\n",
    "    \n",
    "    # ------------------------\n",
    "    # Filter Data and Display Comment Details\n",
    "    # ------------------------\n",
    "    all_ids = set([clicked_node]) | descendant_nodes\n",
    "    subData = global_data['data'][global_data['data']['comment_id'].isin(all_ids)]\n",
    "    print(f\"Performing topic analysis on {len(subData)} comments...\")\n",
    "    \n",
    "    # Build a multi-line string with comment details\n",
    "    details_str = \"\"\n",
    "    for idx_row, row in subData.iterrows():\n",
    "        details_str += f\"comment_id: {row['comment_id']} | serial_number: {row['serial_number']}\\n\"\n",
    "        details_str += f\"Comment: {row['comment_body']}\\n\\n\"\n",
    "    \n",
    "    # Display details in a new figure (simple text display)\n",
    "    fig_details, ax_details = plt.subplots(figsize=(8, 6))\n",
    "    ax_details.text(0.01, 0.99, details_str, va='top', ha='left', fontsize=8, wrap=True)\n",
    "    ax_details.axis('off')\n",
    "    fig_details.canvas.manager.set_window_title(\"Comment Details\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # ------------------------\n",
    "    # Clean and Tokenize the Comment Text\n",
    "    # ------------------------\n",
    "    # Convert comment text to lowercase\n",
    "    corpus = subData['comment_body'].astype(str).str.lower().tolist()\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidfMat = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # ------------------------\n",
    "    # Latent Semantic Analysis (LSA) and Word Cloud\n",
    "    # ------------------------\n",
    "    # Use TruncatedSVD to perform SVD on the TF-IDF matrix\n",
    "    n_components = 5  # Number of topics to extract (can be adjusted)\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    svd.fit(tfidfMat)\n",
    "    # Extract the first latent topic (first column of V)\n",
    "    V = svd.components_.T  # shape: (n_terms, n_components)\n",
    "    topic_weights = V[:, 0]\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Sort words by absolute weight in descending order\n",
    "    sort_idx = np.argsort(np.abs(topic_weights))[::-1]\n",
    "    topN = min(20, len(words))\n",
    "    topWords = words[sort_idx[:topN]]\n",
    "    topWeights = topic_weights[sort_idx[:topN]]\n",
    "    \n",
    "    # Generate a word cloud (using absolute values of weights)\n",
    "    freq_dict = {word: float(abs(weight)) for word, weight in zip(topWords, topWeights)}\n",
    "    wc = WordCloud(width=800, height=400, background_color='white')\n",
    "    wc.generate_from_frequencies(freq_dict)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Latent Semantic Topic for comment_id: {clicked_node}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top words and their weights\n",
    "    print(\"Top words in the extracted topic:\")\n",
    "    for word, weight in zip(topWords, topWeights):\n",
    "        print(f\"{word} (weight: {weight:.3f})\")\n",
    "\n",
    "# ------------------------\n",
    "# 4. Attach the Click Callback to the Figure\n",
    "# ------------------------\n",
    "cid = fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d56d3c-775b-4bc2-aecd-5e4855319190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
